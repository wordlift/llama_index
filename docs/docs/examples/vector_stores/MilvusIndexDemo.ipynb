{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1496f9de",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/MilvusIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b692c73",
   "metadata": {},
   "source": [
    "# Milvus Vector Store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e7787c2",
   "metadata": {},
   "source": [
    "In this notebook we are going to show a quick demo of using the MilvusVectorStore. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f81e2c81",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47264e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "import textwrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b97a89",
   "metadata": {},
   "source": [
    "### Setup OpenAI\n",
    "Lets first begin by adding the openai api key. This will allow us to access openai for embeddings and to use chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3d4e638",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p 'data/paul_graham/'\n",
    "! wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59ff935d",
   "metadata": {},
   "source": [
    "### Generate our data\n",
    "With our LLM set, lets start using the Milvus Index. As a first example, lets generate a document from the file found in the `data/paul_graham/` folder. In this folder there is a single essay from Paul Graham titled `What I Worked On`. To generate the documents we will use the SimpleDirectoryReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 4d28b248-a306-4a42-9ace-8fd88df7e484\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd270925",
   "metadata": {},
   "source": [
    "### Create an index across the data\n",
    "Now that we have a document, we can can create an index and insert the document. For the index we will use a GPTMilvusIndex. GPTMilvusIndex takes in a few arguments:\n",
    "\n",
    "- `uri (str, optional)`: The URI to connect to, comes in the form of \"https://address:port\" if using Milvus or Zilliz Cloud service, or \"path/to/local/milvus.db\" is using a lite local Milvus. Defaults to \"http://localhost:19530\".\n",
    "- `token (str, optional)`: The token for log in. Empty if not using rbac, if using rbac it will most likely be \"username:password\". Defaults to \"\".\n",
    "- `collection_name (str, optional)`: The name of the collection where data will be stored. Defaults to \"llamalection\".\n",
    "- `dim (int, optional)`: The dimension of the embeddings. If it is not provided, collection creation will be done on first insert. Defaults to None.\n",
    "- `embedding_field (str, optional)`: The name of the embedding field for the collection, defaults to DEFAULT_EMBEDDING_KEY.\n",
    "- `doc_id_field (str, optional)`: The name of the doc_id field for the collection, defaults to DEFAULT_DOC_ID_KEY.\n",
    "- `similarity_metric (str, optional)`: The similarity metric to use, currently supports IP and L2. Defaults to \"IP\".\n",
    "- `consistency_level (str, optional)`: Which consistency level to use for a newly created collection. Defaults to \"Strong\".\n",
    "- `overwrite (bool, optional)`: Whether to overwrite existing collection with same name. Defaults to False.\n",
    "- `text_key (str, optional)`: What key text is stored in in the passed collection. Used when bringing your own collection. Defaults to None.\n",
    "- `index_config (dict, optional)`: The configuration used for building the Milvus index. Defaults to None.\n",
    "- `search_config (dict, optional)`: The configuration used for searching the Milvus index. Note that this must be compatible with the index type specified by index_config. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index over the documnts\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"./milvus_demo.db\", dim=1536, overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "### Query the data\n",
    "Now that we have our document stored in the index, we can ask questions against the index. The index will use the data stored in itself as the knowledge base for chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35369eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned about programming on early computers like the IBM 1401, where programs were typed\n",
      "on punch cards, loaded into memory, and run to produce output. Later, with the advent of\n",
      "microcomputers like the TRS-80, the author was able to have a computer directly in front of them,\n",
      "responding to keystrokes in real-time. This shift in technology allowed the author to write simple\n",
      "games, predictive programs, and even a word processor. Additionally, the author explored philosophy\n",
      "in college but found it lacking in ultimate truths, leading to a switch to studying AI, inspired by\n",
      "works like Heinlein's \"The Moon is a Harsh Mistress\" and seeing SHRDLU in a PBS documentary.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99212d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author experienced a difficult moment when their mother had a stroke and was put in a nursing\n",
      "home. The stroke destroyed her balance, and the author and their sister were determined to help her\n",
      "get out of the nursing home and back to her house.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64cc925b",
   "metadata": {},
   "source": [
    "This next test shows that overwriting removes the previous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d641e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: The author is the one who wrote the text or created the content.\n"
     ]
    }
   ],
   "source": [
    "vector_store = MilvusVectorStore(dim=1536, overwrite=True)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    [Document(text=\"The number that is being searched for is ten.\")],\n",
    "    storage_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"Who is the author?\")\n",
    "print(\"Res:\", res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8123529",
   "metadata": {},
   "source": [
    "The next test shows adding additional data to an already existing  index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c429a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: The number is ten.\n"
     ]
    }
   ],
   "source": [
    "del index, vector_store, storage_context, query_engine\n",
    "\n",
    "vector_store = MilvusVectorStore(overwrite=False)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What is the number?\")\n",
    "print(\"Res:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5287c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Paul Graham\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"Who is the author?\")\n",
    "print(\"Res:\", res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
